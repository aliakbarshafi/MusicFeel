{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>mood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRBFHQG128F93092E5.h5</td>\n",
       "      <td>Donnie McClurkin</td>\n",
       "      <td>Psalm 27</td>\n",
       "      <td>2000</td>\n",
       "      <td>One thing have I desired up the Lord\\r\\r\\r\\r\\n...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAKWXS128F930F798.h5</td>\n",
       "      <td>Lollipop Lust Kill</td>\n",
       "      <td>No Answer (Outro)</td>\n",
       "      <td>2002</td>\n",
       "      <td>Though I heard you say you love me\\r\\r\\r\\r\\nIt...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRASWIV128E0788A84.h5</td>\n",
       "      <td>The Smashing Pumpkins</td>\n",
       "      <td>Real Love</td>\n",
       "      <td>2000</td>\n",
       "      <td>Fall in to the century of supersonic cross\\r\\r...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAYTDZ128F93146E3.h5</td>\n",
       "      <td>Stevie Ray Vaughan And Double Trouble</td>\n",
       "      <td>Mary Had A Little Lamb</td>\n",
       "      <td>1988</td>\n",
       "      <td>Mary had a little lamb\\r\\r\\r\\r\\nIt's fleece wa...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAROPS128F92F09A5.h5</td>\n",
       "      <td>Carl Belew</td>\n",
       "      <td>Am I That Easy To Forget</td>\n",
       "      <td>1959</td>\n",
       "      <td>Am I that easy to forget?\\r\\r\\r\\r\\n\\r\\r\\r\\r\\nY...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file                                 artist  \\\n",
       "0  TRBFHQG128F93092E5.h5                       Donnie McClurkin   \n",
       "1  TRAKWXS128F930F798.h5                     Lollipop Lust Kill   \n",
       "2  TRASWIV128E0788A84.h5                  The Smashing Pumpkins   \n",
       "3  TRAYTDZ128F93146E3.h5  Stevie Ray Vaughan And Double Trouble   \n",
       "4  TRAROPS128F92F09A5.h5                             Carl Belew   \n",
       "\n",
       "                      title  year  \\\n",
       "0                  Psalm 27  2000   \n",
       "1         No Answer (Outro)  2002   \n",
       "2                 Real Love  2000   \n",
       "3    Mary Had A Little Lamb  1988   \n",
       "4  Am I That Easy To Forget  1959   \n",
       "\n",
       "                                              lyrics   mood  \n",
       "0  One thing have I desired up the Lord\\r\\r\\r\\r\\n...  happy  \n",
       "1  Though I heard you say you love me\\r\\r\\r\\r\\nIt...    sad  \n",
       "2  Fall in to the century of supersonic cross\\r\\r...  happy  \n",
       "3  Mary had a little lamb\\r\\r\\r\\r\\nIt's fleece wa...  happy  \n",
       "4  Am I that easy to forget?\\r\\r\\r\\r\\n\\r\\r\\r\\r\\nY...    sad  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('../Data/training.csv')\n",
    "df_test= pd.read_csv('../Data/validation.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['happy' 'sad' 'happy' 'happy' 'sad'] ...\n",
      "Encoded: [0 1 0 0 1] ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "X_train = df_train['lyrics'].values \n",
    "y_train = df_train['mood'].values\n",
    "\n",
    "X_test = df_test['lyrics'].values \n",
    "y_test = df_test['mood'].values\n",
    "\n",
    "\n",
    "print('Original: %s ...' %y_train[:5])\n",
    "\n",
    "le_train = LabelEncoder()\n",
    "le_train.fit(y_train)\n",
    "y_train = le_train.transform(y_train)\n",
    "\n",
    "le_test = LabelEncoder()\n",
    "le_test.fit(y_test)\n",
    "y_test=le_test.transform(y_test)\n",
    "\n",
    "print('Encoded: %s ...' %y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Stop words from English Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words ['i', 'me', 'my', 'myself', 'we'] ...\n"
     ]
    }
   ],
   "source": [
    "with open('../stopwords.txt', 'r') as infile:\n",
    "    stop_words = infile.read().splitlines()\n",
    "print('stop words %s ...' %stop_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform texts into bag of words models - Trying different tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The Porter stemming algorithm (or ‘Porter stemmer’) is a process for removing the commoner morphological and inflexional \n",
    "endings from words in English. Its main use is as part of a term normalisation process that is usually done when \n",
    "setting up Information Retrieval systems.\n",
    "\n",
    "Snowball Stemmer: https://snowballstem.org/algorithms/\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = EnglishStemmer()\n",
    "\n",
    "# raw words\n",
    "tokenizer = lambda text: text.split()\n",
    "\n",
    "# words after Porter stemming \n",
    "tokenizer_porter = lambda text: [porter.stem(word) for word in text.split()]\n",
    "\n",
    "# Words after Snowball stemming\n",
    "tokenizer_snowball = lambda text: [snowball.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "DenseTransformer is a simple transformer that converts a sparse into a dense numpy array, \n",
    "It is required for scikit-learn's Pipeline as CountVectorizers are used in combination with estimators \n",
    "that are not compatible with sparse matrices.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "vector_1 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "vector_2 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_porter)\n",
    "    \n",
    "vector_3 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_snowball)  \n",
    "\n",
    "vector_4 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "vector_5 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_porter)\n",
    "    \n",
    "vector_6 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Pipelines are used to Sequentially apply a list of transforms and a final estimator. \n",
    "Intermediate steps of pipeline must implement fit and transform methods. \n",
    "Final estimator implements the fit.\n",
    "\"\"\"\n",
    "\n",
    "pipelines = []\n",
    "vectorizers = [vector_1, vector_2, vector_3, vector_4, vector_5, vector_6]\n",
    "for v in vectorizers:\n",
    "    pipelines.append(Pipeline([('vect', v),\n",
    "                               ('dense', DenseTransformer()),\n",
    "                               ('clf', RandomForestClassifier(n_estimators=100))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sizes\n",
      "\n",
      "CountVec: 8188\n",
      "CountVec porter: 6282\n",
      "CountVec snowball: 6253\n",
      "TfidfVec: 8188\n",
      "TfidfVec porter: 6282\n",
      "TfidfVec snowball: 6253\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary sizes\\n')\n",
    "labels = ['CountVec', 'CountVec porter', 'CountVec snowball','TfidfVec', 'TfidfVec porter', 'TfidfVec snowball']\n",
    "\n",
    "for label, v in zip(labels, vectorizers):\n",
    "    v.fit(X_train)\n",
    "    print('%s: %s' % (label, len(v.vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf 1, CountVec: 67.74744623655916\n",
      "clf 2, CountVec porter: 67.68660394265234\n",
      "clf 3, CountVec snowball: 69.46052867383513\n",
      "clf 4, TfidfVec: 66.58351254480286\n",
      "clf 5, TfidfVec porter: 68.47804659498209\n",
      "clf 6, TfidfVec snowball: 69.73920250896057\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "\n",
    "labels = ['CountVec', 'CountVec porter', 'CountVec snowball','TfidfVec', 'TfidfVec porter', 'TfidfVec snowball']\n",
    "\n",
    "\n",
    "\n",
    "dic = {'Data':labels,\n",
    "     'ROC AUC (%)':[],}\n",
    "\n",
    "for i,clf in enumerate(pipelines):\n",
    "    scores = cross_validation.cross_val_score(estimator=clf, X=X_train, y=y_train, scoring='roc_auc', cv=10)\n",
    "    print('clf %s, %s: %s' % (i+1, labels[i], scores.mean()*100))\n",
    "    dic['ROC AUC (%)'].append('%0.2f (+/- %0.2f)' % (scores.mean()*100, scores.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import KFold\n",
    "from scipy import interp\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "classifier = Pipeline([('vect', TfidfVectorizer(binary=False,\n",
    "                                             stop_words=stop_words,\n",
    "                                             ngram_range=(1,1),\n",
    "                                             preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                                             max_features = 5000,\n",
    "                                             tokenizer=lambda text: [porter.stem(word) for word in text.split()])),\n",
    "                        ('dense', DenseTransformer()),\n",
    "                        ('clf', RandomForestClassifier(n_estimators=100))])\n",
    "\n",
    "\n",
    "crossValidation = KFold(X_train.shape[0], n_folds=5)\n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "all_tpr = []\n",
    "\n",
    "\n",
    "for i,(train, test) in enumerate(crossValidation):\n",
    "    probabilities = classifier.fit(X_train[train], y_train[train]).predict_proba(X_train[test])\n",
    "\n",
    "    # Compute ROC curve and area under the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_train[test], probabilities[:, 1])\n",
    "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i+1, roc_auc))\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random Guessing')\n",
    "\n",
    "mean_tpr /= len(crossValidation)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--',\n",
    "         label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Images/roc_RandomForest_TfPorter.png', dpi=300)\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter Tuning:  Finding best parameters through GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(binary=False,\n",
    "                       stop_words=stop_words,\n",
    "                       ngram_range=(1,1),\n",
    "                       preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                       tokenizer=lambda text: [porter.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosing the number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf 1, 50: 64.50 (+/- 5.70)\n",
      "clf 2, 100: 68.25 (+/- 5.21)\n",
      "clf 3, 200: 71.28 (+/- 6.86)\n",
      "clf 4, 400: 69.35 (+/- 6.14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import cross_validation\n",
    "\n",
    "pipe_1 = Pipeline([\n",
    "                ('vect',   vect),\n",
    "                ('dense', DenseTransformer()),\n",
    "                ('clf', RandomForestClassifier(n_estimators=50))])\n",
    "\n",
    "pipe_2 = Pipeline([\n",
    "                ('vect',   vect),\n",
    "                ('dense', DenseTransformer()),\n",
    "                ('clf', RandomForestClassifier(n_estimators=100))])\n",
    "\n",
    "pipe_3 = Pipeline([\n",
    "                ('vect',   vect),\n",
    "                ('dense', DenseTransformer()),\n",
    "                ('clf', RandomForestClassifier(n_estimators=200))])\n",
    "\n",
    "pipe_4 = Pipeline([\n",
    "                ('vect',   vect),\n",
    "                ('dense', DenseTransformer()),\n",
    "                ('clf', RandomForestClassifier(n_estimators=400))])\n",
    "\n",
    "labels = [50, 100, 200, 400]\n",
    "\n",
    "for i,clf in enumerate([pipe_1, pipe_2, pipe_3, pipe_4]):\n",
    "    scores = cross_validation.cross_val_score(estimator=clf, X=X_train, y=y_train, scoring='roc_auc', cv=10)\n",
    "    print('clf %s, %s: %0.2f (+/- %0.2f)' % (i+1, labels[i], scores.mean()*100, scores.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feature = vect.fit_transform(X_train)\n",
    "#X_train_feature = X_train_feature.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\n",
      "Grid scores:\n",
      "\n",
      "0.697 (+/-0.034) for {'criterion': 'gini', 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.675 (+/-0.028) for {'criterion': 'gini', 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "0.709 (+/-0.039) for {'criterion': 'gini', 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.690 (+/-0.033) for {'criterion': 'gini', 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 3}\n",
      "0.690 (+/-0.026) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.699 (+/-0.025) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "0.685 (+/-0.036) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.682 (+/-0.029) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3}\n",
      "0.690 (+/-0.034) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.679 (+/-0.032) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "0.694 (+/-0.038) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.696 (+/-0.039) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 3}\n",
      "0.704 (+/-0.034) for {'criterion': 'entropy', 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.696 (+/-0.025) for {'criterion': 'entropy', 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "0.687 (+/-0.040) for {'criterion': 'entropy', 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.693 (+/-0.032) for {'criterion': 'entropy', 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 3}\n",
      "0.696 (+/-0.035) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.688 (+/-0.030) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "0.679 (+/-0.023) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.684 (+/-0.035) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3}\n",
      "0.693 (+/-0.034) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.694 (+/-0.035) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "0.712 (+/-0.030) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.699 (+/-0.032) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "classifier_1 = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "\n",
    "parameters = [\n",
    "  {'criterion': ['gini', 'entropy'], \n",
    "   'max_features': ['auto', 'log2', 'sqrt'],\n",
    "   'min_samples_split':[2,3], \n",
    "   'min_samples_leaf':[1,2]},\n",
    " ]\n",
    "\n",
    "\n",
    "gridSearch_1 = GridSearchCV(classifier_1, \n",
    "                           parameters, \n",
    "                           n_jobs=1, \n",
    "                           scoring='roc_auc',\n",
    "                           cv=10)\n",
    "\n",
    "gridSearch_1.fit(X_train_feature, y_train)\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print()\n",
    "print(gridSearch_1.best_estimator_)\n",
    "print()\n",
    "print(\"Grid scores:\")\n",
    "print()\n",
    "for params, mean_score, scores in gridSearch_1.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "            % (mean_score, scores.std() / 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# `pos_label` for positive class, since we have sad=1, happy=0\n",
    "\n",
    "acc_scorer = metrics.make_scorer(metrics.accuracy_score, greater_is_better=True)\n",
    "pre_scorer = metrics.make_scorer(metrics.precision_score, greater_is_better=True, pos_label=0)\n",
    "rec_scorer = metrics.make_scorer(metrics.recall_score, greater_is_better=True, pos_label=0)\n",
    "f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, pos_label=0)\n",
    "auc_scorer = metrics.make_scorer(metrics.roc_auc_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Train CountVec', 'Train CountVec porter', 'Train CountVec snowball',\n",
    "          'Train TfidfVec', 'Train TfidfVec porter', 'Train TfidfVec snowball', \n",
    "          'Test CountVec', 'Test CountVec porter', 'Test CountVec snowball', \n",
    "          'Test TfidfVec', 'Test TfidfVec porter', 'Test TfidfVec snowball']\n",
    "\n",
    "dic = {'Data':labels,\n",
    "     'ACCURACY (%)':[],\n",
    "     'PRECISION (%)':[],\n",
    "     'RECALL (%)':[],\n",
    "     'F1 (%)':[],\n",
    "     'ROC AUC (%)':[],\n",
    "}\n",
    "\n",
    "\n",
    "for clf in pipelines:\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "for clf in pipelines:\n",
    "\n",
    "    dic['ACCURACY (%)'].append(acc_scorer(estimator=clf, X=X_train, y_true=y_train))\n",
    "    dic['PRECISION (%)'].append(pre_scorer(estimator=clf, X=X_train, y_true=y_train))\n",
    "    dic['RECALL (%)'].append(rec_scorer(estimator=clf, X=X_train, y_true=y_train))\n",
    "    dic['F1 (%)'].append(f1_scorer(estimator=clf, X=X_train, y_true=y_train))\n",
    "    dic['ROC AUC (%)'].append(auc_scorer(estimator=clf, X=X_train, y_true=y_train))\n",
    "\n",
    "for clf in pipelines:\n",
    "\n",
    "    dic['ACCURACY (%)'].append(acc_scorer(estimator=clf, X=X_test, y_true=y_test))\n",
    "    dic['PRECISION (%)'].append(pre_scorer(estimator=clf, X=X_test, y_true=y_test))\n",
    "    dic['RECALL (%)'].append(rec_scorer(estimator=clf, X=X_test, y_true=y_test))\n",
    "    dic['F1 (%)'].append(f1_scorer(estimator=clf, X=X_test, y_true=y_test))\n",
    "    dic['ROC AUC (%)'].append(auc_scorer(estimator=clf, X=X_test, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACCURACY (%)</th>\n",
       "      <th>PRECISION (%)</th>\n",
       "      <th>RECALL (%)</th>\n",
       "      <th>F1 (%)</th>\n",
       "      <th>ROC AUC (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train CountVec</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train CountVec porter</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train CountVec snowball</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train TfidfVec</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train TfidfVec porter</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train TfidfVec snowball</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test CountVec</th>\n",
       "      <td>64.0</td>\n",
       "      <td>57.14</td>\n",
       "      <td>40.0</td>\n",
       "      <td>47.06</td>\n",
       "      <td>60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test CountVec porter</th>\n",
       "      <td>69.0</td>\n",
       "      <td>64.52</td>\n",
       "      <td>50.0</td>\n",
       "      <td>56.34</td>\n",
       "      <td>65.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test CountVec snowball</th>\n",
       "      <td>67.0</td>\n",
       "      <td>61.29</td>\n",
       "      <td>47.5</td>\n",
       "      <td>53.52</td>\n",
       "      <td>63.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test TfidfVec</th>\n",
       "      <td>70.0</td>\n",
       "      <td>67.86</td>\n",
       "      <td>47.5</td>\n",
       "      <td>55.88</td>\n",
       "      <td>66.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test TfidfVec porter</th>\n",
       "      <td>68.0</td>\n",
       "      <td>64.29</td>\n",
       "      <td>45.0</td>\n",
       "      <td>52.94</td>\n",
       "      <td>64.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test TfidfVec snowball</th>\n",
       "      <td>66.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>51.43</td>\n",
       "      <td>62.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ACCURACY (%)  PRECISION (%)  RECALL (%)  F1 (%)  \\\n",
       "Train CountVec                  100.0         100.00       100.0  100.00   \n",
       "Train CountVec porter           100.0         100.00       100.0  100.00   \n",
       "Train CountVec snowball         100.0         100.00       100.0  100.00   \n",
       "Train TfidfVec                  100.0         100.00       100.0  100.00   \n",
       "Train TfidfVec porter           100.0         100.00       100.0  100.00   \n",
       "Train TfidfVec snowball         100.0         100.00       100.0  100.00   \n",
       "Test CountVec                    64.0          57.14        40.0   47.06   \n",
       "Test CountVec porter             69.0          64.52        50.0   56.34   \n",
       "Test CountVec snowball           67.0          61.29        47.5   53.52   \n",
       "Test TfidfVec                    70.0          67.86        47.5   55.88   \n",
       "Test TfidfVec porter             68.0          64.29        45.0   52.94   \n",
       "Test TfidfVec snowball           66.0          60.00        45.0   51.43   \n",
       "\n",
       "                         ROC AUC (%)  \n",
       "Train CountVec                100.00  \n",
       "Train CountVec porter         100.00  \n",
       "Train CountVec snowball       100.00  \n",
       "Train TfidfVec                100.00  \n",
       "Train TfidfVec porter         100.00  \n",
       "Train TfidfVec snowball       100.00  \n",
       "Test CountVec                  60.00  \n",
       "Test CountVec porter           65.83  \n",
       "Test CountVec snowball         63.75  \n",
       "Test TfidfVec                  66.25  \n",
       "Test TfidfVec porter           64.17  \n",
       "Test TfidfVec snowball         62.50  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance = pd.DataFrame(dic)\n",
    "performance = performance[['ACCURACY (%)', 'PRECISION (%)', 'RECALL (%)', 'F1 (%)', 'ROC AUC (%)']]\n",
    "performance.index=(labels)\n",
    "performance = performance*100\n",
    "performance = np.round(performance, decimals=2)\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.to_csv('../Data/randomForest_performance.csv', index_label=False, float_format='%2.2f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
